---
title: "Code Light / Fluo"
output:
  html_document:
    df_print: paged
date: "12-05-2025"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, include=FALSE}
library(ncdf4) 
library(raster) 
library(ggplot2)
library(dplyr)
library(lubridate)
library(data.table)
library(oce)
library(terra)
library(tidyterra)
library(cmocean)
library(stringr)
library(zoo)
library(stats)
library(sf)
library(ggnewscale)

```

<h2>Global Variables</h2>
```{r variables}
MAX_CHLA_DEPTH <- 175       # define depth at which to consider dark signal (CHLA "absolute zero")
SURFACE_NOT_NAN <- 4:10

DDU_loc=NULL
DDU_loc$lat = -66.66306	
DDU_loc$lon = 140.00083
DDU_loc=as.data.frame(DDU_loc)

# 66°39’47”S, 140°00’03”E

```

<h2>Metadata analysis</h2>
<h4>Date interval for each individual</h4>
```{r}
ROOT_PROJ <- "/home/cactus/Documents/Oceano/M2/LOCEAN/Weddell_seal_fluo"
DATA_FOLDER <- "oceanographic_data"
DEPLOY_FOLDER <- "wd10"
DATA_PATH <- file.path(ROOT_PROJ, DATA_FOLDER, DEPLOY_FOLDER)

nc_files <- list.files(DATA_PATH, pattern = "\\.nc$", full.names = TRUE)
ranges_list <- list()

for (nc_file in nc_files) {
  nc_data <- nc_open(nc_file)
  juld <- ncvar_get(nc_data, "JULD")

  dateTime <- as.POSIXct(juld * 86400, origin = "1950-01-01", tz = "UTC")
  dateTime <- round(dateTime, units = "mins")

  ranges_list[[basename(nc_file)]] <- range(dateTime, na.rm = TRUE)
  nc_close(nc_data)
}

for (file_name in names(ranges_list)) {
  cat("\nFichier:", file_name, "\n")
  print(ranges_list[[file_name]])
}
```
<h4>Number of profiles per individual</h4>
```{r}
ROOT_PROJ <- "/home/cactus/Documents/Oceano/M2/LOCEAN/Weddell_seal_fluo"
DATA_FOLDER <- "oceanographic_data"
DEPLOY_FOLDER <- "wd11"
DATA_PATH <- file.path(ROOT_PROJ, DATA_FOLDER, DEPLOY_FOLDER)

nc_files <- list.files(DATA_PATH, pattern = "\\.nc$", full.names = TRUE)
profiles_list <- list()

for (nc_file in nc_files)
{
  nc_data <- nc_open(nc_file)

  profiles_data <- ncvar_get(nc_data, "LIGHT")

  num_profiles <- dim(profiles_data)[2]

  profiles_list[[basename(nc_file)]] <- num_profiles

  nc_close(nc_data)
}

for (file_name in names(profiles_list)) {
  cat("\nFichier:", file_name, "\n")
  cat("Nombre de profils de lumière:", profiles_list[[file_name]], "\n")
}

```

<h2>Paths and NetCDF extraction</h2>
<h4>Paths</h4>
```{r paths}
ROOT_PROJ <- "/home/cactus/Documents/Oceano/M2/LOCEAN/Weddell_seal_fluo"

DATA_FOLDER <- "oceanographic_data"
DEPLOY_FOLDER <- "wd11"
SEAL_NAME <- paste0(DEPLOY_FOLDER,"-682-17")
SEAL_FILE <- paste0(SEAL_NAME, "_hr2_prof.nc")
NC_PATH <- file.path(ROOT_PROJ, DATA_FOLDER, DEPLOY_FOLDER, SEAL_FILE)

BATHY_FOLDER <- "bathy"
FINE_TOPO_FILE <- "gvdem100v3/w001001.adf"
LARGE_TOPO_FILE <- "gebco_2023_n-50.0_s-79.0_w90.0_e180.0.tif"
BATHY_PATH_FINE <- file.path(ROOT_PROJ, BATHY_FOLDER, FINE_TOPO_FILE)
BATHY_PATH_LARGE <- file.path(ROOT_PROJ, BATHY_FOLDER, LARGE_TOPO_FILE)

FAST_ICE_FOLDER <- "fast_ice"
FAST_ICE_FILE <- "mertz_sara_akiko_19.nc"
FAST_ICE_PATH <- file.path(ROOT_PROJ, FAST_ICE_FOLDER, FAST_ICE_FILE)

AMSR_FOLDER <- "amsr2"
AMSR_FOLDER_YEAR <- "2019"
AMSR_FOLDER_PATH <- file.path(ROOT_PROJ,AMSR_FOLDER,AMSR_FOLDER_YEAR)
AMSR_FILE <- "asi-AMSR2-s3125-20190401-v5.4.tif"
AMSR_PATH <- file.path(ROOT_PROJ,AMSR_FOLDER,AMSR_FOLDER_YEAR,AMSR_FILE)
```

<h4>NetCDF Extract</h4>
```{r nc-code}
nc_data <- nc_open(NC_PATH)
# print(nc_data)
lon   <- ncvar_get(nc_data, "LONGITUDE")
lat   <- ncvar_get(nc_data, "LATITUDE")
chla  <- ncvar_get(nc_data, "CHLA")
light <- ncvar_get(nc_data, "LIGHT")
temp  <- ncvar_get(nc_data, "TEMP_ADJUSTED")
sal   <- ncvar_get(nc_data, "PSAL_ADJUSTED")
pres  <- ncvar_get(nc_data, "PRES_ADJUSTED")
juld  <- ncvar_get(nc_data, "JULD")
```

<h2>Date and coordinates correction</h2>
<h4>Date formatting</h4>
```{r dataframe}
pres_vec <- pres[, 1]

dateTime <- as.POSIXct(juld * 86400, origin = "1950-01-01", tz = "UTC")
dateTime <- round(dateTime, units = "mins")  # arrondi à la minute la plus proche
```

<h4>Correcting longitude and latitude</h4>
```{r corrected lon lat}
diag <- read.csv("diag_2017-2024_forclaude_07apr24.csv", stringsAsFactors = FALSE)
diag$date <- as.POSIXct(diag$date, format="%Y-%m-%d %H:%M")

# extraire l'individu qui nous intéresse et trier par date
df_wd11 <- diag %>% filter(id == SEAL_NAME)
df_wd11 <- df_wd11 %>% arrange(date)

t1 <- as.numeric(df_wd11$date )
t2 <- as.numeric(as.POSIXct(dateTime))

cols <- c("lat", "lon")
interp_results <- lapply(cols, function(col) {
  approx(x = t1, y = df_wd11[[col]], xout = t2, rule = 2)$y
})

names(interp_results) <- paste0(cols, "_corr")
df_interp <- as.data.frame(interp_results)

df_original <- data.frame(lon = lon, lat = lat)

ggplot() +
  geom_point(data = df_original, aes(x = lon, y = lat, color = "Original"), alpha = 0.4, size = 2) +
  geom_point(data = df_interp, aes(x = lon_corr, y = lat_corr, color = "Interpolated"), alpha = 0.4, size = 2) +
  scale_color_manual(name = "Type de données", values = c("Original" = "blue", "Interpolated" = "red")) +
  labs(title = paste0(SURFACE_NOT_NAN[1], " to ", SURFACE_NOT_NAN[length(SURFACE_NOT_NAN)], " m deep"), x = "Longitude", y = "Latitude") +
  theme_minimal()

lon <- df_interp$lon_corr
lat <- df_interp$lat_corr
```

<h2>Computing Mixed Layer Depth</h2>
```{r MLD}

# Fonction pour faire comme find(..., 1, 'first') de MATLAB
first_valid_index <- function(condition) 
{
  idx <- which(condition)
  if (length(idx) > 0) return(idx[1]) else return(NA)
}

n_profiles <- ncol(sal)
n_levels <- nrow(sal)

sigma0_mat <- matrix(NA, nrow = n_levels, ncol = n_profiles)
mld_vec <- rep(NA, n_profiles)
densThreshold <- 0.03
refDepth <- 10
minProfileStartDepth <- 20

# Trouver les profils valides selon leur première profondeur
profileStartDepth <- rep(NA, n_profiles)
for (i in 1:n_profiles) {
  firstGood <- first_valid_index(!is.na(temp[, i]) & !is.na(pres[, i]) & !is.na(sal[, i]))
  if (!is.na(firstGood) && !is.na(pres[firstGood, i])) {
    profileStartDepth[i] <- pres[firstGood, i]
  }
}
validProfiles <- which(profileStartDepth <= minProfileStartDepth)

# Boucle sur les profils valides
for (i in validProfiles) {
  S <- sal[, i]
  T <- temp[, i]
  P <- pres[, i]
  
  good <- !is.na(S) & !is.na(T) & !is.na(P)
  if (sum(good) == 0) next  # ignorer les profils entièrement vides
  
  # Calcul de la densité potentielle
  ctd <- as.ctd(salinity = S[good], temperature = T[good], pressure = P[good])
  sigma0 <- swSigma0(ctd@data$salinity, ctd@data$temperature, ctd@data$pressure)

  full_sigma0 <- rep(NA, n_levels)
  full_sigma0[which(good)] <- sigma0
  sigma0_mat[, i] <- full_sigma0

  # Trouve l'indice de référence (premier >= 10 m)
  refIdx <- first_valid_index(ctd@data$pressure >= refDepth)
  
  if (!is.na(refIdx) && !is.na(sigma0[refIdx])) {
    refSigma0 <- sigma0[refIdx]
    # Différence de densité sur tous les niveaux valides
    deltaSigma <- full_sigma0 - refSigma0

    # Trouve l'indice où le seuil est dépassé au-delà de 10 m
    mld_idx <- first_valid_index(deltaSigma > densThreshold & P > refDepth)
    if (!is.na(mld_idx)) {
      mld_vec[i] <- P[mld_idx]
    } else {
      mld_vec[i] <- NA
    }
  }
}

# Créer un data frame avec les valeurs valides
df_mld <- data.frame(
  profile = validProfiles,
  value = mld_vec[validProfiles]
)

# Graphique ggplot
ggplot(df_mld, aes(x = profile, y = value)) +
  geom_point(shape = 4, color = "blue") +
  geom_smooth(aes(y = value), method = "loess", formula = "y ~ x", color = "red", linewidth = 0.25, se = FALSE) +
  scale_y_reverse() +
  labs(x = "Profil", y = "MLD (m)", title = "Profondeur de la couche mélangée (MLD)") +
  theme_minimal()

```

<h4> Light and fluo profiles, not corrected </h4>
```{r}
# idprofile <- 13
# 
# df_L_F <- data.frame(
#   depth = pres_vec[1:MAX_CHLA_DEPTH],
#   chla = chla[1:MAX_CHLA_DEPTH,idprofile],
#   light = light[1:MAX_CHLA_DEPTH,idprofile]
# )
# 
# # long format
# df_long <- df_L_F %>%
#   pivot_longer(cols = c("chla", "light"), names_to = "variable", values_to = "value")
# 
# # Tracé avec ggplot
# ggplot(df_long, aes(x = value, y = depth, color = variable)) +
#   geom_line(size = 1) +
#   scale_y_reverse() +  # profondeur vers le bas
#   labs(x = "Valeur", y = "Profondeur (m)", color = "Variable") +
#   theme_minimal()

library(ggplot2)
library(dplyr)

idprofile <- 11

# Base commune
depth <- pres_vec[1:MAX_CHLA_DEPTH]
chla_vals <- chla[1:MAX_CHLA_DEPTH, idprofile]
light_vals <- light[1:MAX_CHLA_DEPTH, idprofile]

# Échelle de conversion : on ramène "light" sur une échelle comparable visuellement
light_scaled <- light_vals / max(light_vals, na.rm = TRUE)
chla_scaled <- chla_vals / max(chla_vals, na.rm = TRUE)

df_plot <- data.frame(
  depth = depth,
  chla = chla_vals,
  light = light_vals,
  light_scaled = light_scaled,
  chla_scaled = chla_scaled
)

ggplot(df_plot, aes(y = depth)) +
  geom_line(aes(x = chla_scaled), color = "green", size = 1) +
  geom_line(aes(x = light_scaled), color = "black", size = 1) +
  scale_y_reverse() +
  scale_x_continuous(
    name = "Chlorophylle (mg/m³)",
    sec.axis = sec_axis(~ . , name = "Lumière (uE/m²/s)")  # même échelle, visuellement différent
  ) +
  theme_minimal() +
  theme(
    axis.title.x.top = element_text(color = "black"),
    axis.title.x.bottom = element_text(color = "green"),
    axis.text.x.top = element_text(color = "black"),
    axis.text.x.bottom = element_text(color = "green")
  )


```

<h2>LIGHT</h2>
<h4>Estimating the number of empty light profiles</h4>
```{r NaColumns}
col_na_only <- apply(light, 2, function(col) all(is.na(col)))

cat("Number of only N/A columns :", sum(col_na_only), "\n\n")
```

<h4>Computing Dark value for light profile</h4>
```{r DarkValue}
library(nortest)

# first_valid_index <- function(vec) which(!is.na(vec))[1]
# last_valid_index  <- function(vec) tail(which(!is.na(vec)), 1)
# 
# correct_light_profile <- function(light_non_log_col, light_log_col, pres_vec, 
#                                   first_non_NaN_temp, last_non_NaN_temp,
#                                   min_botIntegBound = 150, max_depth_dataset = length(pres_vec),
#                                   alpha = 0.01, delta_for_cstLIGHT = 1e-8) 
# {
#   if (all(is.na(light_non_log_col))) 
#   {
#     return(rep(NA, length(light_non_log_col)))
#   }
#   
#   # Initialiser les vecteurs pour stocker les résultats
#   light_corrected <- light_non_log_col
#   PDARK_temp <- NA
#   VDARK_temp <- 0
#   slop_temp <- NA
#   slopPART_temp <- NA
#   h_temp <- 1  # par défaut : non normal
#   
#   # Vérification de la présence de données suffisantes
#   if ((last_non_NaN_temp - first_non_NaN_temp + 1) >= 2) {
#     # Détection signal constant
#   }else{
#     for (jj in first_non_NaN_temp:(last_non_NaN_temp - 1)) {
#       range_temp <- diff(range(light_corrected[jj:last_non_NaN_temp], na.rm = TRUE))
#       if (!is.na(range_temp) && range_temp <= delta_for_cstLIGHT) {
#         light_corrected[(jj+1):length(light_corrected)] <- NA
#         last_non_NaN_temp <- jj
#         break
#       }
#     }
# 
#     # Test de normalité
#     for (jj in first_non_NaN_temp:(last_non_NaN_temp - 3)) {
#       segment <- na.omit(light_corrected[jj:last_non_NaN_temp])
#       if (length(segment) >= 4) {
#         test <- tryCatch(nortest::lillie.test(segment), error = function(e) NULL)
#         if (!is.null(test) && test$p.value > alpha) {
#           h_temp <- 0
#           break
#         }
#       }
#     }
# 
#     if (h_temp == 1) {
#       ind_dark <- last_non_NaN_temp
#     } else {
#       ind_dark <- jj
#       VDARK_temp <- median(light_corrected[ind_dark:last_non_NaN_temp], na.rm = TRUE)
#     }
#     PDARK_temp <- pres_vec[ind_dark]
# 
#     # Calcul pente
#     slop_temp <- (light_log_col[ind_dark] - light_log_col[first_non_NaN_temp]) / 
#                  (pres_vec[ind_dark] - pres_vec[first_non_NaN_temp])
#     if (ind_dark > min_botIntegBound) {
#       slopPART_temp <- (light_log_col[min_botIntegBound] - light_log_col[first_non_NaN_temp]) / 
#                        (pres_vec[min_botIntegBound] - pres_vec[first_non_NaN_temp])
#     }
# 
#     # Correction du profil
#     if (h_temp == 0) {
#       light_corrected[ind_dark:length(light_corrected)] <- NA
#     }
#     light_corrected <- light_corrected - VDARK_temp
#     light_corrected[light_corrected <= 0] <- NA
# 
#     # Interpolation
#     if (sum(!is.na(light_corrected)) >= 2) {
#       light_corrected[first_non_NaN_temp:last_non_NaN_temp] <- 
#         zoo::na.approx(light_corrected[first_non_NaN_temp:last_non_NaN_temp], na.rm = FALSE)
#     }
#     light_corrected[light_corrected < 0] <- 0
#   }
# 
#   return(light_corrected)
# }
# 
# light_corrected_non_log <- apply(light_non_log, 2, function(col) {
#   correct_light_profile(
#     light_non_log_col = exp(col),
#     light_log_col = col,
#     pres_vec = pres_vec,
#     first_non_NaN_temp = first_valid_index(col),
#     last_non_NaN_temp = last_valid_index(col)
#   )
# })

light_non_log <- exp(light)
light_log <- light
min_window = 4
alpha = 0.01

light_corrected_non_log <- light_non_log
# light_corrected_non_log_temp <- light_non_log

# Trouver les colonnes non vides dans `light`
col_na_only <- apply(light_non_log, 2, function(col) all(is.na(col)))
lightNonNan_non_log <- light_non_log[,!col_na_only]
valid_cols <- which(!col_na_only)

# Initialiser les vecteurs pour stocker les résultats
PCST_vec <- rep(NA, ncol(lightNonNan_non_log))
PDARK_vec <- rep(NA, ncol(lightNonNan_non_log))
VDARK_vec <- rep(NA, ncol(lightNonNan_non_log))
delta_for_cstLIGHT <- 1e-8 # define standard delta between min and max of profile to eliminate profile for being a constant profile
max_depth <- nrow(light_non_log)

# Boucle sur les indices des colonnes non vides
for (ii_temp in valid_cols) {

  # Initialiser les variables locales
  PCST_temp <- MAX_CHLA_DEPTH
  PDARK_temp <- MAX_CHLA_DEPTH
  VDARK_temp <- 0

  # Indices des premières et dernières valeurs non NA dans le profil
  col_temp <- light_non_log[, ii_temp]
  first_non_NaN_temp <- which(!is.na(col_temp))[1]
  last_non_NaN_temp  <- tail(which(!is.na(col_temp)), 1)
  
  # initialize cstDetect_temp used in cst test
  cstDetect_temp <- 0

  if ((last_non_NaN_temp - first_non_NaN_temp + 1) < 2) {
    # Pas assez de points valides pour faire un test
    ind_cst_temp <- last_non_NaN_temp + 1
    PCST_temp <- NA
  } else {
    for (jj_temp in first_non_NaN_temp:(last_non_NaN_temp - 1)) {
      # Calcul de l'amplitude (range) dans la partie profonde
      range_temp <- diff(range(light_corrected_non_log[jj_temp:last_non_NaN_temp], na.rm = TRUE))

      if (is.na(range_temp) || range_temp <= delta_for_cstLIGHT) {

        # Signal considéré comme constant
        cstDetect_temp <- 1
        break # end for loop when range < delta_for_cstLIGHT (1e-8) i.e. LIGHT signal is considered constant in interval j:bottom
      }
    }
    # write index of first value belonging to dark signal
    ind_cst_temp <- jj_temp + 1
    # write depth of dark value
    PCST_temp <- pres_vec[ind_cst_temp]
  }

  # Si signal constant détecté, on masque la partie cst (du fond vers le haut)
  if (cstDetect_temp == 1) {
    # On s’assure de ne pas dépasser la profondeur max
    start_idx <- min(ind_cst_temp + 1, max_depth)
    light_corrected_non_log[start_idx:max_depth, ii_temp] <- NA
    last_non_NaN_temp <- ind_cst_temp - 1
  }

  h_temp <- 1 # Par défaut : signal considéré non normal (i.e., pas sombre)

  if ((last_non_NaN_temp - first_non_NaN_temp + 1) < 4) {
    # Pas assez de points pour un test valide
    ind_dark_temp <- last_non_NaN_temp + 1
    PDARK_temp <- NA
    VDARK_temp <- NA
    slop_temp <- NA
    slopPART_temp <- NA
  } else {
    # Sampled vector X must have at least 4 valid observations
    for (jj_temp in first_non_NaN_temp:(last_non_NaN_temp - 3)) {

      # Sous-échantillon du profil lumière pour test de normalité
      light_segment <- light_corrected_non_log[jj_temp:last_non_NaN_temp, ii_temp]
      light_segment <- light_segment[!is.na(light_segment)]

      # Test de normalité (Lilliefors ~ lillie.test dans package 'nortest')
      if (length(light_segment) >= 4) {
        test_result <- tryCatch(lillie.test(light_segment), error = function(e) NULL)

        if (!is.null(test_result) && test_result$p.value > 0.01) {
          h_temp <- 0  # Le segment suit une distribution normale : considéré sombre
          break
        }
      }
    }

    # Déterminer l'indice et la profondeur associée au signal sombre
    if (h_temp == 1) {
      # Aucun test de normalité valide → prendre la dernière valeur
      ind_dark_temp <- last_non_NaN_temp
      PDARK_temp <- pres_vec[ind_dark_temp]
      # VDARK_temp reste à 0 (déjà initialisé)
    } else {
      # Normalité détectée : on garde l'indice de cassure
      ind_dark_temp <- jj_temp
      PDARK_temp <- pres_vec[ind_dark_temp]
      VDARK_temp <- median(light_corrected_non_log[ind_dark_temp:last_non_NaN_temp, ii_temp], na.rm = TRUE)
    }

    topdark_temp <- floor(pres_vec[ind_dark_temp]) ;
    botdark_temp <- floor(pres_vec[last_non_NaN_temp]) + 1 ;

    # Calcul de la pente d'atténuation totale
    slop_temp <- (light_log[ind_dark_temp, ii_temp] - light_log[first_non_NaN_temp, ii_temp]) / (pres_vec[ind_dark_temp] - pres_vec[first_non_NaN_temp])

    # Calcul de la pente partielle sur un intervalle d'intégration (150 = minimum lower boundary (bottom) required for profiles to be imported in linear functional model)
    if (ind_dark_temp > 150) {
      slopPART_temp <- (light_log[150, ii_temp] - light_log[first_non_NaN_temp, ii_temp]) / (pres_vec[min_botIntegBound] - pres_vec[first_non_NaN_temp])
    } else {
      slopPART_temp <- NA
    }
  }

  # ============================
  # Correction du profil LIGHT
  # ============================

  # Indices à mettre à NaN (zones non fiables)
  toNaN_temp <- unique(c(1:max(1, first_non_NaN_temp - 1), seq(min(ind_dark_temp + 1, MAX_CHLA_DEPTH, last_non_NaN_temp + 1), length(light_corrected_non_log[, ii_temp]))))

  # Suppression du signal sombre si détecté (h_temp = 0)
  if (h_temp == 0) {
    light_corrected_non_log[ind_dark_temp:nrow(light_corrected_non_log), ii_temp] <- NA
  }

  # Soustraction de la valeur sombre (dark value)
  light_corrected_non_log[, ii_temp] <- light_corrected_non_log[, ii_temp] - VDARK_temp

  # Remplacement des valeurs négatives par NaN
  light_corrected_non_log[light_corrected_non_log[, ii_temp] <= 0, ii_temp] <- NA

  # Interpolation linéaire si au moins 2 valeurs valides
  n_values_temp <- sum(!is.na(light_corrected_non_log[, ii_temp]))
  if (n_values_temp >= 2) {
    vect_temp <- light_corrected_non_log[first_non_NaN_temp:last_non_NaN_temp, ii_temp]
    vect_temp <- zoo::na.approx(vect_temp, na.rm = FALSE)
    light_corrected_non_log[first_non_NaN_temp:last_non_NaN_temp, ii_temp] <- vect_temp
    light_corrected_non_log[toNaN_temp, ii_temp] <- NA
  }

  # # Copie du profil corrigé pour affichage (profil final sans bruit sombre)
  # light_corrected_non_log_temp[, ii_temp] <- light_corrected_non_log_temp[, ii_temp] - VDARK_temp
  # # Valeurs négatives à 0
  # light_corrected_non_log_temp[light_corrected_non_log_temp[, ii_temp] < 0, ii_temp] <- 0
}
# light_corrected_non_log <- apply(light_non_log, 2, function(col) correct_light_profile(col, light, pres_vec))
light_corrected_log <- log(pmax(light_corrected_non_log, 1e-10))

```

```{r}

  
#   
#   
#   
#   
#   
#   # indices valides
#   idx <- which(!is.na(light_non_log) & light_non_log != 0)
#   if (length(idx) < min_window) return(rep(NA, length(light_non_log)))
#   
#   last_non_na <- max(idx)
#   first_non_na <- min(idx)
# 
#   # balayage depuis la surface vers le fond pour chaque valeur
#   dark_idx <- last_non_na
#   for (j in seq(first_non_na, last_non_na - (min_window - 1))) 
#   {
#     window <- light_non_log[j:last_non_na]
#     if (length(window) >= min_window) 
#     {
#       # p.value
#       test_result <- tryCatch(shapiro.test(window), error = function(e) list(p.value = 0))
#       if (!is.null(test_result$p.value) && test_result$p.value > alpha) 
#       {
#         # si significtaive entre la valeur non nulle et le fond, à la premiere on prend cette indice là
#         dark_idx <- j
#         break
#       }
#     }
#   }
# 
#   # dark value = médiane des valeurs identifiées comme dark noise
#   dark_vals <- light_non_log[dark_idx:last_non_na]
#   dark_val <- median(dark_vals, na.rm = TRUE)
#   dark_val <- round(dark_val, 4)
#   
#   # cat(paste("dark_val : ", dark_val, "\n\n"))
# 
#   # correction du profil
#   corrected <- light_non_log - dark_val
# 
#   # valeurs négatives à NA
#   corrected[corrected <= 0] <- NA
#   
#   # interpolation linéaire
#   if (sum(!is.na(corrected)) >= 2) {
#     corrected <- na.approx(corrected, x = pres_vec, na.rm = FALSE)
#   }
#   return(corrected)
# }

# exp aux valeurs de lumière (en log) car pas de calculs en log (incorrect)
```

<h4>Quenching depth, based on PAR15</h4>
```{r Quenching depth}
# compute depth of quenching threshold 15 umol.m-2.s-1 (xing et al. 2018)
quenchDepth_vec <- apply(light_corrected, 2, function(col) {
  idx <- tail(which(col > 15), 1)
  if (length(idx) == 0) return(NA) else return(idx)
})
```

<h4>Plotting one light profile, before and after dark value correction</h4>
```{r LightProfilePlot}

id_profile = 1

plot(light_non_log[1:MAX_CHLA_DEPTH, id_profile], pres_vec[1:MAX_CHLA_DEPTH], type = "l", col = "blue",
     ylim = rev(range(pres_vec[1:MAX_CHLA_DEPTH])), xlim = c(0, 3e-4),
     xlab = "Light ln(µmol/m²/sec)", ylab = "Depth (m)", main = paste0("Light profile ",id_profile," - Original and Corrected\n", SEAL_NAME))

lines(light_corrected_non_log[1:MAX_CHLA_DEPTH, id_profile], pres_vec[1:MAX_CHLA_DEPTH], col = "red")
legend("bottomright", legend = c("Original", "Corrected"),col = c("blue", "red"), lty = 1, bty = "n")

abline(h = quenchDepth_vec[id_profile], col = "cyan", lty = 2)
abline(h = df_mld$value[id_profile], col = "purple", lty = 2)

legend("bottomleft", legend = c("Quenching Depth", "MLD"), col = c("cyan", "purple"), lty = 1, bty="n")


```

<h4>Plotting all light profiles, before and after dark value correction</h4>
```{r PlotAllProfiles-original&Corrected}
matplot(light[1:MAX_CHLA_DEPTH,], pres_vec[1:MAX_CHLA_DEPTH],
        type = "l",
        ylim = rev(c(0, MAX_CHLA_DEPTH)),
        lty = 1,
        xlab = "Light ln(µmol/m²/s)",
        ylab = "Depth (m)",
        main = paste0("Light profiles : ", SEAL_NAME))
grid()

matplot(light_corrected_log[1:MAX_CHLA_DEPTH,], pres_vec[1:MAX_CHLA_DEPTH],
        type = "l",
        ylim = rev(c(0, MAX_CHLA_DEPTH)),
        lty = 1,
        xlab = "Light ln(µmol/m²/s)",
        ylab = "Depth (m)",
        main = paste0("Light profiles corrected : ", SEAL_NAME))
grid()

```

<h4>Total light for each profile</h4>
```{r Total-Light}
light_subset <- light_corrected_log[1:MAX_CHLA_DEPTH, !col_na_only] # sesf031_light_data_preprocess1_nonNan2 : erasing NaN profiles

total_light <- colSums(light_subset, na.rm = TRUE)
profil_names <- as.character(seq_along(total_light))

plot(
  total_light,
  type = "b",                     
  pch = 18,                        
  col = "blue",
  xlab = "Profil n°",
  ylab = "Total lumière (µmol/m²/s)",
  main = paste0("Lumière totale par profile : ", SEAL_NAME),
  xaxt = "n"                       
)

# axis(1, at = seq(1, length(profil_names), by = 20), labels = profil_names[seq(1, length(profil_names), by = 20)])
```

<h4>Saving and classifying light profiles (condition does not seem strong enough)</h4>
```{r SaveAndClassifyLightProfiles}

dir_standard = "profils_light_standard"
dir_atypique = "profils_light_atypique"
dir.create(dir_standard, showWarnings = FALSE)
dir.create(dir_atypique, showWarnings = FALSE)

nlight = ncol(light_corrected)

for (i in 1:nlight) {
  profil_light <- light_corrected[, i]
  profil_pres <- pres_vec
  
  delta_light <- profil_light - profil_light[1]

  if (any(delta_light > 0, na.rm = TRUE)) {
    class_dir <- dir_atypique
  } else {
    class_dir <- dir_standard
  }
  
  filename <- paste0(class_dir, "/profil_light", i, ".png")
  
  png(filename, width = 800, height = 600)
  
  plot(profil_light, profil_pres,
       type = "l",
       xlim = c(-10,10),
       ylim = rev(c(0, MAX_CHLA_DEPTH)),
       xlab = "Light (µmol/m²/s)",
       ylab = "Press",
       main = paste0("Light profile ",i," - Original and Corrected\n", SEAL_NAME))
  grid()
  
  abline(h = quenchDepth_vec[i], col = "cyan", lty = 2)
  abline(h = df_mld$value[i], col = "purple", lty = 2)

  legend("topright", legend = c("Quenching Depth", "MLD"), col = c("cyan", "purple"), lty = 2, cex = 0.8, bg = "white")
  
  dev.off()
}

# cat("Classification terminée :\n")
# cat(nlight, "profils analysés\n")
# cat("->", length(list.files(dir_standard)), "profils standards dans", dir_standard, "\n")
# cat("->", length(list.files(dir_atypique)), "profils atypiques dans", dir_atypique, "\n\n")

```

<h2>FLUO</h2>
<h4>FLUO : Estimation of NA columns</h4>
```{r NaN_Chla}
col_na_only <- apply(chla, 2, function(col) all(is.na(col)))
which(col_na_only)

cat("Number of only N/A columns :", sum(col_na_only), "\n\n")
```

<h4>Dark-value correction for Chl-a profiles (only works for some profiles, investigating where the code is incorrect)</h4>
```{r DarkValue_chla}
correct_chla_profile_matlab <- function(col_data, pres_vec,
                                        darkDepthDelta_temp = 10,
                                        delta_for_cstCHLA = 0.01)
{
  max_depth <- nrow(col_data)  # utile pour construire les matrices
  n_prof <- ncol(col_data)

  # Vérifier que le profil atteint la profondeur désirée
  if (MAX_CHLA_DEPTH > max_depth) {
    stop("Le profil ne descend pas jusqu'à MAX_CHLA_DEPTH.")
  }

  # Indices de la tranche profonde à analyser
  deep_indices <- (MAX_CHLA_DEPTH - (darkDepthDelta_temp - 1)) : MAX_CHLA_DEPTH
  
  # Initialiser la matrice des valeurs CHLA corrigées
  CHLA_nadRegDk <- matrix(NA, nrow = max_depth, ncol = n_prof)
  darkDepth_vec <- rep(NA, n_prof)

  for (ii in 1:n_prof) {
    profile <- col_data[, ii]

    if (all(is.na(profile))) {
      next  # Profil entièrement vide
    }

    # Calcul de l'offset à partir des profondeurs profondes
    chla_deep <- profile[deep_indices]
    dark_val <- median(chla_deep, na.rm = TRUE)

    if (is.na(dark_val)) {
      dark_val <- 0  # Valeur de secours si tout est NA
    }

    profile_corrected <- profile - dark_val
    profile_corrected[profile_corrected < 0] <- 0  # Forcer à 0 les négatifs

    CHLA_nadRegDk[, ii] <- profile_corrected

    # Détection de plateau constant profond (PDARK: où la chlorophylle est constante)
    idx_non_na <- which(!is.na(profile_corrected))
    if (length(idx_non_na) < 2) {
      darkDepth_vec[ii] <- NA
      next
    }

    first_idx <- idx_non_na[1]
    last_idx <- tail(idx_non_na, 1)

    PDARK_temp <- MAX_CHLA_DEPTH + 1

    for (jj in first_idx:(last_idx - 1)) {
      # Calcul de l'amplitude maximale de la CHLA du point jj jusqu'à la fin du profil
      amplitude <- suppressWarnings(max(profile_corrected[jj:last_idx], na.rm = TRUE))
      # Si l'amplitude est très faible (donc profil plat) ou non définie (NA), on considère que la zone sombre commence ici
      if (is.na(amplitude) || amplitude <= delta_for_cstCHLA) {
        idx_dark_temp <- jj + 1
        # Mise à zéro de toutes les valeurs CHLA à partir de cette profondeur (zone sombre détectée)
        profile_corrected[idx_dark_temp:last_idx] <- 0
        PDARK_temp <- pres_vec[idx_dark_temp]
        break
      }
    }
    CHLA_nadRegDk[, ii] <- profile_corrected
    darkDepth_vec[ii] <- PDARK_temp
  }

  return(list(
    corrected = CHLA_nadRegDk,
    darkDepth = darkDepth_vec
  ))
}


result_list <- lapply(seq_len(ncol(chla)), function(i) {
  correct_chla_profile_matlab(chla[, i, drop = FALSE], pres_vec)
})

```

<h4>Plotting dark value correction for Chl-a</h4>
```{r plot_dk_chla}
# Extraire les résultats
chla_corrected <- sapply(result_list, function(x) x$corrected)   # matrice de profils corrigés
dark_vals <- sapply(result_list, function(x) x$dark_val)         # vecteur
PDARK_vals <- sapply(result_list, function(x) x$PDARK_temp)      # vecteur

# sauvegardés dans une structure de type data.frame
chlaData <- data.frame(
  darkValProfile = dark_vals,
  darkDepth = PDARK_vals
)

nprofile=5

# Plot only one of the 150
plot(chla[1:MAX_CHLA_DEPTH, nprofile], pres_vec[1:MAX_CHLA_DEPTH], type = "l", col = "blue",
     ylim = rev(range(pres_vec[1:MAX_CHLA_DEPTH], na.rm = TRUE)),
     xlim = range(c(0,2)), #range(c(chla[1:MAX_CHLA_DEPTH, nprofile], chla_corrected[1:MAX_CHLA_DEPTH, nprofile])),
     xlab = "Chl-a (mg/m³)", ylab = "Depth (m)", main = paste("Chl-a profile (DK)", nprofile, "-", SEAL_NAME))

lines(chla_corrected[1:MAX_CHLA_DEPTH, nprofile], pres_vec[1:MAX_CHLA_DEPTH], col = "red")

legend("bottomright", legend = c("Original", "Corrected"), col = c("blue", "red"), lty = 1, bty = "n")

# abline(h = quenchDepth_vec[nprofile], col = "cyan", lty = 2)
abline(h = df_mld$value[nprofile], col = "purple", lty = 2)

legend("bottomleft", legend = "MLD", col =  "purple", lty = 1, bty="n")
```

<h4>Plotting all dark value correction profiles for Chl-a, before and after correction</h4>
```{r plot_all_dk_chla}
matplot(chla[1:MAX_CHLA_DEPTH,], pres_vec[1:MAX_CHLA_DEPTH],
        type = "l",
        ylim = rev(c(0, MAX_CHLA_DEPTH)),
        lty = 1,
        xlab = "Chl-a (mg/m³)",
        ylab = "Depth (m)",
        main = paste0("Chl-a profiles : ", SEAL_NAME))
grid()

matplot(chla_corrected[1:MAX_CHLA_DEPTH,], pres_vec[1:MAX_CHLA_DEPTH],
        type = "l",
        ylim = rev(c(0, MAX_CHLA_DEPTH)),
        lty = 1,
        xlab = "Chl-a (mg/m³)",
        ylab = "Depth (m)",
        main = paste0("Chl-a profiles corrected : ", SEAL_NAME))
grid()
```

```{r}
chla <- chla_corrected
```

<h4>Non Photochemical Quenching correction for Chl-a profiles (as dark-value correction works only for some profiles, need to verify this code once DK is fixed)</h4>
```{r NPQ_correction}

apply_npq_correction <- function(chla, pres_vec, mld_vec, quenchDepth_vec, MAX_CHLA_DEPTH) {
  nprofiles <- ncol(chla)
  chla_corrected <- chla  # initialiser avec les valeurs originales
  npqCorrDepth <- rep(NA_real_, nprofiles)  # index (non profondeur) de correction
  
  # Profils valides : chl-a non vide + quenching connu
  idx_chlaNonNan <- apply(!is.na(chla), 2, any)
  idx_lightNonNan <- !is.na(quenchDepth_vec)
  valid_profiles <- which(idx_chlaNonNan & idx_lightNonNan)
  
  for (nprofile in valid_profiles) {
    profile_chla <- chla[1:MAX_CHLA_DEPTH, nprofile]
    MLD <- mld_vec[nprofile]
    quenchDepth <- quenchDepth_vec[nprofile]
    
    if (is.na(MLD) || is.na(quenchDepth)) next
    
    # Déterminer la profondeur limite de correction
    depthLimit <- min(MLD, quenchDepth, na.rm = TRUE)
    indexDepthLimit <- which.min(abs(pres_vec[1:MAX_CHLA_DEPTH] - depthLimit))
    
    # Chercher la valeur max de fluorescence dans cette couche
    surface_chla <- profile_chla[1:indexDepthLimit]
    if (all(is.na(surface_chla))) next
    
    chlMaxValue <- max(surface_chla, na.rm = TRUE)
    indexMaxFluorescence <- which.max(surface_chla)
    
    # Appliquer la correction NPQ jusqu'à (et y compris) l'index du maximum
    profile_chla[1:indexMaxFluorescence] <- chlMaxValue
    
    # Stocker les résultats
    chla_corrected[1:MAX_CHLA_DEPTH, nprofile] <- profile_chla
    npqCorrDepth[nprofile] <- indexMaxFluorescence  # Optionnel : pres_vec[...] si vous préférez la profondeur réelle
  }
  
  return(list(chla_corrected = chla_corrected,
              npqCorrDepth = npqCorrDepth))
}


result <- apply_npq_correction(chla, pres_vec, mld_vec, quenchDepth_vec, MAX_CHLA_DEPTH)

chla_corrected <- result$chla_corrected
npqCorrDepth <- result$npqCorrDepth


# Pour chaque profil de Chl-a valide (non NA):
#     Calculer la profondeur limite de correction (depthLimit) :
#        min entre la profondeur de mélange (MLD) et la profondeur de quenching (quenchDepth_vec)
# 
#     Déterminer l’indice (indexDepthLimit) dans le vecteur de pression correspondant à cette profondeur limite
# 
#     Identifier, dans la couche de surface jusqu’à cette profondeur :
#         l’intensité maximale de fluorescence (chlMaxValue)
#         et sa position (indexMaxFluorescence)
# 
#     Corriger les valeurs de Chl-a à toutes les profondeurs au-dessus de indexMaxFluorescence :
#         Remplacer par la valeur maximale détectée (chlMaxValue)
# 
#     Mettre à jour la profondeur de correction effective si nécessaire

# CONDITIONS PARTICULIERES
# % NPQ LAYER MIGHT BE THINNER
# flags_temp(exp(lightData.subsurVal) < 15) = 1 ;
# % SHALLOW MIXING CONDITIONS
# flags_temp(genData.MLDphy(ii_temp) <= lightData.quenchDepth_vec) = 2 ;
# % NIGHT PROFILE
# flags_temp(idx035_lightDay == 0) = 7 ;
# % BOTTOM OF NPQ LAYER NOT REACHED
# flags_temp(isnan(lightData.quenchDepth_vec)) = 8 ;
# % NO LIGHT DATA
# flags_temp(idx031_lightNonNan == 0) = 9 ;
# % OK
# flags_temp(isnan(flags_temp)) = 0 ;

```

<h4>Plotting NPQ correction for Chl-a</h4>
```{r NPQ_plot}
nprofile <- 5

plot(
  chla[1:MAX_CHLA_DEPTH, nprofile],
  pres_vec[1:MAX_CHLA_DEPTH],
  type = "l", col = "blue",
  ylim = rev(range(pres_vec[1:MAX_CHLA_DEPTH], na.rm = TRUE)),
  xlim = range(c(0,2)),#range(c(chla[1:MAX_CHLA_DEPTH, nprofile], chla_corrected[1:MAX_CHLA_DEPTH, nprofile]), na.rm = TRUE),
  xlab = "Chl-a", ylab = "Depth",
  main = paste("Profil", nprofile, "-", SEAL_NAME)
)

lines(chla_corrected[1:MAX_CHLA_DEPTH, nprofile], pres_vec[1:MAX_CHLA_DEPTH], col = "red")

legend("bottomright", legend = c("Original", "Corrected"), col = c("blue", "red"), lty = 1, bty = "n")

abline(h = quenchDepth_vec[nprofile], col = "cyan", lty = 2)
abline(h = df_mld$value[nprofile], col = "purple", lty = 2)

legend("bottomleft", legend = c("Quenching Depth", "MLD"), col = c("cyan", "purple"), lty = 1, bty="n")

```

<h4>Computing total fluo per profile (same remark as for light, plot it before and after correction</h4>
```{r TotalChla}
chla_subset <- chla_corrected [1:MAX_CHLA_DEPTH, !col_na_only]

total_chla <- colSums(chla_subset, na.rm = TRUE)
profil_names <- as.character(seq_along(total_chla))

plot(
  total_chla,
  type = "b",                     
  pch = 18,                        
  col = "blue",
  xlab = "Profil n°",
  ylab = "Total Chl-a (mg/m³)",
  main = paste0("Chl-a totale par profile : ", SEAL_NAME),
  xaxt = "n"                       
)

axis(1, at = seq(1, length(profil_names), by = 20), labels = profil_names[seq(1, length(profil_names), by = 20)])
```

<h4>Saving and classifying Chl-a profiles</h4>
```{r SaveAndClassifyChlaProfiles}

dir_standard = "profils_chla_standard"
dir_atypique = "profils_chla_atypique"
dir.create(dir_standard, showWarnings = FALSE)
dir.create(dir_atypique, showWarnings = FALSE)

nchla = ncol(chla)

for (i in 1:nchla) {
  profil_chla <- chla[, i]
  
  delta_chla <- profil_chla - profil_chla[1]

  if (any(delta_chla > 0, na.rm = TRUE)) {
    class_dir <- dir_atypique
  } else {
    class_dir <- dir_standard
  }
  
  filename <- paste0(class_dir, "/profil_chla", i, ".png")
  
  png(filename, width = 800, height = 600)
  
  plot(profil_chla, pres_vec,
       type = "l",
       xlim = c(0,0.1),
       ylim = rev(c(0, MAX_CHLA_DEPTH)),
       xlab = "Chl-a (mg/m³)",
       ylab = "Press",
       main = paste("Profil chla - Col", i))
  grid()
  
  abline(h = quenchDepth_vec[i], col = "cyan", lty = 2)
  abline(h = df_mld$value[i], col = "purple", lty = 2)

  legend("bottomleft", legend = c("Quenching Depth", "MLD"), col = c("cyan", "purple"), lty = 1, bty="n")
  
  dev.off()
}

# cat("Classification terminée :\n")
# cat(nchla, "profils analysés\n")
# cat("->", length(list.files(dir_standard)), "profils standards dans", dir_standard, "\n")
# cat("->", length(list.files(dir_atypique)), "profils atypiques dans", dir_atypique, "\n\n")

```

<h2>Light and fluo corrected : profiles</h2>
```{r}
idprofile <- 11

df_L_F_corrected <- data.frame(
  depth = pres_vec[1:MAX_CHLA_DEPTH],
  chla = chla_corrected[1:MAX_CHLA_DEPTH,idprofile],
  light = light_corrected[1:MAX_CHLA_DEPTH,idprofile]
)

# long format
df_long_corrected <- df_L_F_corrected %>%
  pivot_longer(cols = c("chla", "light"), names_to = "variable", values_to = "value")

# Tracé avec ggplot
ggplot(df_long_corrected, aes(x = value, y = depth, color = variable)) +
  geom_line(size = 1) +
  scale_y_reverse() +  # profondeur vers le bas
  labs(x = "Valeur", y = "Profondeur (m)", color = "Variable") +
  theme_minimal()

```

<h2>Light Temperature Salinity Fluo</h2>
<h4>Plotting one Light temperature salinity & FLUO profile</h4>
```{r PlotProfile_LTSC}
n <- 12

# Création du data.frame long
df_profile <- data.frame(
  depth = pres_vec,
  Light = light[, n],
  Temperature = temp[, n],
  Salinity = sal[, n],
  CHLA = chla[, n]
) %>% pivot_longer(cols = -depth, names_to = "variable", values_to = "value")

# ggplot avec facet
ggplot(df_profile, aes(x = value, y = depth)) +
  geom_line() +
  scale_y_reverse(limits = c(MAX_CHLA_DEPTH, 0)) +
  facet_wrap(~ variable, scales = "free_x", nrow = 1) +
  labs(x = NULL, y = "Depth (m)") +
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12, face = "bold"),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = "grey80")) +
    ggtitle(paste0(SEAL_NAME, " : ", n, "/", ncol(light_corrected), " (", format(dateTime[n], "%Y-%m-%d"), ")"))

```

```{r dataframeCreation}
# 
# depth_interval <- pres_vec[SURFACE_NOT_NAN]
# light_interval <- light_corrected[SURFACE_NOT_NAN, ]
# 
# depth_diff <- c(diff(depth_interval), 0)  
# 
# light_surf_avg <- apply(light_interval, 2, function(x) {
#   sum(x * depth_diff, na.rm = TRUE) / sum(depth_diff, na.rm = TRUE)
# })
# 
# light_20m <- light_corrected[20, ]
# light_40m <- light_corrected[40, ]
# 
# df_light <- data.frame(
#   lon = lon,
#   lat = lat,
#   date = dateTime,
#   light_surf = light_surf_avg,
#   light_20 = light_20m,
#   light_40 = light_40m
# )
# 
# df_10m_clean <- df_light %>% filter(!is.na(light_surf))
# df_20m_clean <- df_light %>% filter(!is.na(light_20))
# df_40m_clean <- df_light %>% filter(!is.na(light_40))
# 
# n_profiles_surf <- length(df_10m_clean$light_surf)
# n_profiles_20 <- length(df_20m_clean$light_20)
# n_profiles_40 <- length(df_40m_clean$light_40)
# 
# ggplot(df_10m_clean, aes(x = lon, y = lat, color = light_surf)) +
#   geom_point(size = 2) +
#   scale_color_viridis_c(limits = c(0, 10), oob = scales::squish) +
#   labs(
#     title = paste0(n_profiles_surf," light profiles (", SURFACE_NOT_NAN[1], " to ",SURFACE_NOT_NAN[7],"m deep)"),
#     x = "Longitude",
#     y = "Latitude",
#     color = "Light\nln(µmol/m²/s)"
#   ) +
#   theme_minimal()
# 
# ggplot(df_20m_clean, aes(x = lon, y = lat, color = light_20)) +
#   geom_point(size = 2) +
#   scale_color_viridis_c(limits = c(0, 10), oob = scales::squish) +
#   labs(
#     title = paste0(n_profiles_20," light profiles (", 20,"m deep)"),
#     x = "Longitude",
#     y = "Latitude",
#     color = "Light\nln(µmol/m²/s)"
#   ) +
#   theme_minimal()
# 
# ggplot(df_40m_clean %>% filter(!is.na(light_40)), aes(x = lon, y = lat, color = light_40)) +
#   geom_point(size = 2) +
#   scale_color_viridis_c(limits = c(0, 10), oob = scales::squish) +
#   labs(
#     title = paste0(n_profiles_40, " light profiles (", 40, "m deep)"),
#     x = "Longitude",
#     y = "Latitude",
#     color = "Light\nln(µmol/m²/s)"
#   ) +
#   theme_minimal()
# 
```

<h2>MAPS</h2>
<h4>Preparing coordinates for Fast ice / SIC map</h4>
```{r setWindow_fastIce_SIC}
LON_MIN <- 130
LON_MAX <- 150
LAT_MIN <- -68
LAT_MAX <- -65
```

<h4>Loading bathymetry</h4>
```{r Bathy}
fine_topo <- raster(BATHY_PATH_FINE)
# fine_topo[fine_topo >=0] <- NA
fine_topo_spat <- terra::rast(fine_topo)

large_topo <- raster(BATHY_PATH_LARGE)
large_topo[large_topo >=0] <-NA
large_topo_spat <- terra::rast(large_topo)

fine_topo_spat <- crop(fine_topo_spat, extent(LON_MIN, LON_MAX, LAT_MIN, LAT_MAX))
large_topo_spat <- crop(large_topo_spat, extent(LON_MIN, LON_MAX, LAT_MIN, LAT_MAX))
```

<h4>Loading and cropping fast ice raster</h4>
```{r Fast_ice}
prj <- "+proj=stere +lat_0=-90 +lat_ts=-70 +datum=WGS84"
ex <- c(-2691055, 2933945, -2390156, 2309844)

fast_ice_raster <- rast(FAST_ICE_PATH, "Fast_Ice_Time_Series")
set.ext(fast_ice_raster, ext(ex))
set.crs(fast_ice_raster, crs(prj))
#plot(fast_ice_raster[[1]], legend = FALSE) # le continent

points <- cbind(
  lon = c(LON_MIN, LON_MAX),  
  lat = c(LAT_MIN, LAT_MAX)
)

prjpoints <- project(points, to = prj, from = "EPSG:4326")
new_ex<-extent(prjpoints[2,1],prjpoints[1,1],prjpoints[2,2],prjpoints[1,2])
new_r=crop(fast_ice_raster,new_ex)
# From feb to may
time_index <- seq(1,4, by = 1) 
x_sub <- subset(new_r, time_index)
plot(x_sub, legend = FALSE)
```

<h4>Removing values from the raster we don't want (we want fast-ice values)</h4>
```{r fast_ice_processing}
x_sub2= project(x_sub, crs(large_topo_spat), 
                res = res(large_topo_spat)) 

# Create a sequence of months between the first and last month
start <- format(as.Date("2019-02-05"), "%m")
end <- format(as.Date("2019-05-06"), "%m")

months_seq <- seq(from = start, to = end)

# Extract the month names from the sequence
month_names <- paste0("2019 - ", month.name[months_seq])

names(x_sub2) <- month_names
plot(x_sub2, legend = FALSE)
x_sub2 <- crop(x_sub2, extent(LON_MIN, LON_MAX, LAT_MIN, LAT_MAX))
plot(x_sub2, legend = FALSE)

x_sub2=round(x_sub2
             )
continent <- x_sub2
continent[x_sub2 != 1] <- NA
continent[x_sub2 == 1] <- 1

glacier <- x_sub2
glacier[x_sub2 != 4] <- NA
glacier[x_sub2 == 4] <- 1

```

<h4>Loading and cropping AMSR raster</h4>
```{r AMSR}
wgs <- "+proj=longlat +ellps=WGS84"
 amsr_raster <- raster(AMSR_PATH)
 amsr_raster[amsr_raster > 100] <- NA_real_
 amsr_raster <- setValues(raster(amsr_raster), values(amsr_raster))
 plot(amsr_raster)
 
 points <- cbind(
   lon = c(LON_MIN, LON_MAX),  
   lat = c(LAT_MIN, LAT_MAX)
 )
 
 boxext <- extent(projectExtent(raster(extent(c(LON_MIN,LON_MAX,LAT_MIN,LAT_MAX)), crs = wgs), projection(amsr_raster)))
 amsr_cropped=crop(amsr_raster, boxext)
 plot(amsr_cropped)
 
 amsr_raster_cropped <- projectRaster(amsr_cropped, crs = wgs)
 plot(amsr_raster_cropped)
```

<h4>Computing mean/sd/median of SIC raster map, and plotting</h4>
```{r AMSR_processing}

wgs <- "+proj=longlat +ellps=WGS84"
amsr_dir <- AMSR_FOLDER_PATH
all_files <- list.files(amsr_dir, full.names = TRUE)

# on cible une date et un mois, pour ensuite en calculer la moyenne, mediane et sd
target_year_month <- "201901"
date_obj <- as.Date(paste0(target_year_month, "01"), format = "%Y%m%d")
formatted_title <- format(date_obj, "%B %Y") 

# on selectionne tous les fichiers du mois
selected_files <- all_files[str_detect(all_files, paste0("s3125-", target_year_month))]

raster_list <- lapply(selected_files, function(f) {
  r <- raster(f)
  r[r > 100] <- NA_real_
  r
})

raster_stack <- stack(raster_list)
ref_raster <- raster_list[[1]]

points <- cbind(lon = c(LON_MIN, LON_MAX), lat = c(LAT_MIN, LAT_MAX))

# Recadrage
boxext <- extent(projectExtent(raster(extent(c(LON_MIN, LON_MAX, LAT_MIN, LAT_MAX)), crs = wgs), projection(ref_raster)))
raster_cropped <- crop(raster_stack, boxext)

mean_raster <- calc(raster_cropped, fun = mean, na.rm = TRUE)
median_raster <- calc(raster_cropped, fun = median, na.rm = TRUE)
sd_raster <- calc(raster_cropped, fun = sd, na.rm = TRUE)

mean_raster_wgs <- projectRaster(mean_raster, crs = wgs)
median_raster_wgs <- projectRaster(median_raster, crs = wgs)
sd_raster_wgs <- projectRaster(sd_raster, crs = wgs)

mean_spat <- rast(mean_raster_wgs)
median_spat <- rast(median_raster_wgs)
sd_spat <- rast(sd_raster_wgs)

plot(mean_spat, main = paste("AMSR mean -", formatted_title))
plot(median_spat, main = paste("AMSR mediane -", formatted_title))
plot(sd_spat, main = paste("AMSR  SD -", formatted_title))
```
<h4>Dataframe creation for analyzing profiles position</h4>
```{r}
prj <- "+proj=stere +lat_0=-90 +lat_ts=-70 +datum=WGS84"
ex <- c(-2691055, 2933945, -2390156, 2309844)

fast_ice_raster <- rast(FAST_ICE_PATH, "Fast_Ice_Time_Series")
set.ext(fast_ice_raster, ext(ex))
set.crs(fast_ice_raster, crs(prj))

prjpoints <- project(points, to = prj, from = "EPSG:4326")
new_ex<-extent(prjpoints[2,1],prjpoints[1,1],prjpoints[2,2],prjpoints[1,2])
new_r=crop(fast_ice_raster,new_ex)
# From feb to may
time_index <- seq(1,4, by = 1) 
x_sub <- subset(new_r, time_index)

x_sub2= project(x_sub, crs(large_topo_spat), 
                res = res(large_topo_spat)) 

start <- format(as.Date("2019-02-05"), "%m")
end <- format(as.Date("2019-05-06"), "%m")

months_seq <- seq(from = start, to = end)

month_names <- paste0("2019 - ", month.name[months_seq])

names(x_sub2) <- month_names
x_sub2 <- crop(x_sub2, extent(LON_MIN, LON_MAX, LAT_MIN, LAT_MAX))

x_sub2=round(x_sub2)
#description: Classified surface type: 0 = pack ice or ocean; 1 = continent; 2 = islands; 3 = ice shelf; 4 = fast ice; 5 = manual fast ice edge; 6 = auto fast ice edge.
x_sub2[!(x_sub2 == 1 | x_sub2 == 2)] <- NA

continent = x_sub2[[1]]

depth_interval <- pres_vec[SURFACE_NOT_NAN]
light_interval <- light_corrected[SURFACE_NOT_NAN, ]

depth_diff <- c(diff(depth_interval), 0)

light_surf_avg <- apply(light_interval, 2, function(x) {
  sum(x * depth_diff, na.rm = TRUE) / sum(depth_diff, na.rm = TRUE)
})

light_20m <- light_corrected[20, ]
light_40m <- light_corrected[40, ]

df_light <- data.frame(
  lon = lon,
  lat = lat,
  date = dateTime,
  light_surf = light_surf_avg,
  light_20 = light_20m,
  light_40 = light_40m
)

df_10m_clean <- df_light %>% filter(!is.na(light_surf))
df_20m_clean <- df_light %>% filter(!is.na(light_20))
df_40m_clean <- df_light %>% filter(!is.na(light_40))

n_profiles_surf <- length(df_10m_clean$light_surf)
n_profiles_20 <- length(df_20m_clean$light_20)
n_profiles_40 <- length(df_40m_clean$light_40)
```

<h4>Fusion of SIC fast ice and light profiles</h4>
```{r Draft map SIC FI}
month_profile = 12 # december
# table profiles_per_month se base sur df_10m_clean
profiles_per_month <- df_10m_clean %>%
  filter(lubridate::month(date) == month_profile) %>%
  mutate(
    month = lubridate::month(date),
    mld = mld_vec[1:nrow(.)],  # Assurer que mld_vec a la même longueur que le subset
    qd = quenchDepth_vec[1:nrow(.)]  # Pareil pour quenchDepth_vec
  )

continent_factor <- as.factor(continent[[1]])  # Convertit la couche en facteur
glacier_factor <- as.factor(glacier[[1]])  # Convertit la couche en facteur

ggplot() +
  geom_spatraster_contour(data = fine_topo_spat) + # bathy
  geom_spatraster_contour_text(data = fine_topo_spat) +
  geom_spatraster(data = mean_spat, alpha = 0.8) + # moyenne AMSR2 sur le mois
  scale_fill_viridis_c(na.value = NA) +  # NA rendus transparents
  new_scale_fill() +
  geom_spatraster(data = glacier_factor, show.legend = FALSE) +
  scale_fill_manual(values = c("1" = "#A9D6E5"), na.value = NA) +  # Continent en gris, NA en blanc
  new_scale_fill() +
  geom_spatraster(data = continent_factor, show.legend = FALSE) +
  scale_fill_manual(values = c("1" = "slategray"), na.value = NA) +  # Continent en gris, NA en blanc
  geom_point(data = profiles_per_month, aes(x = lon, y = lat), shape = 21, color = "black", fill = "yellow", size = 1) +
  geom_point(aes(x = DDU_loc$lon, y = DDU_loc$lat), color = "red", shape = 4, size = 3, stroke = 1) +  # DDU
  annotate("text", x = DDU_loc$lon, y = DDU_loc$lat, label = "DDU", hjust = -0.2, vjust = -0.5, color = "red", size = 4) +  # DDU label
  xlab("Longitude") + ylab("Latitude") +
  theme_minimal() 
```

<h4>Fusion of SIC fast ice and light profiles (smaller crop)</h4>
```{r SIC_fast_icePlot}

continent_factor <- as.factor(continent[[1]])  # Convertit la couche en facteur
glacier_factor <- as.factor(glacier[[1]])  # Convertit la couche en facteur

ggplot() +
  geom_spatraster_contour(data = fine_topo_spat, color = "black") + # bathy
  geom_spatraster_contour_text(data = fine_topo_spat)+
  
  new_scale_fill() +
  geom_spatraster(data = mean_spat, alpha = 0.8) + # moyenne AMSR2 sur le mois
  scale_fill_viridis_c(na.value = NA) +  # NA rendus transparents
  
  new_scale_fill() +
  geom_spatraster(data = glacier_factor,alpha = 0.9, show.legend = FALSE) +
  scale_fill_manual(values = c("1" = "#A9D6E5"), na.value = NA) +  # Continent en gris, NA en blanc
  
  new_scale_fill() +
  geom_spatraster(data = continent_factor,alpha = 0.4, show.legend = FALSE) +
  scale_fill_manual(values = c("1" = "slategray"), na.value = NA) +  # Continent en gris, NA en blanc
  
  new_scale_fill() +
  geom_point(data = df_10m_clean, aes(x = lon, y = lat, color = light_20m), size = 2) +
  scale_color_viridis_c(limits = c(0.05, 0.1), oob = scales::squish, na.value = NA) +
  geom_point(aes(x = DDU_loc$lon, y = DDU_loc$lat), color = "red", shape = 4, size = 3, stroke = 1) +  # DDU
  annotate("text", x = DDU_loc$lon, y = DDU_loc$lat, label = "DDU", hjust = -0.2, vjust = -0.5, color = "red", size = 4) +  # DDU label
  
  xlab("Longitude") + ylab("Latitude") +
  ggtitle("Carte combinée de la topographie, glace et luminosité près de DDU") +
  theme_minimal() +
  coord_sf(xlim = c(139, 141),ylim = c(-66.8, -66))

```
<h2>Analyzing profiles prosition & solar angle computation</h2>
```{r}
continent_factor <- as.factor(continent)  # Convertit la couche en facteur

ggplot() +
  geom_spatraster_contour(data = fine_topo_spat) + # bathy
  geom_spatraster_contour_text(data = fine_topo_spat) +
  geom_spatraster(data = continent_factor, show.legend = FALSE) +
  scale_fill_manual(values = c("1" = "slategray"), na.value = NA) +  # Continent en gris, NA en blanc
  new_scale_fill() +
  geom_spatraster(data = glacier_factor,alpha = 0.9, show.legend = FALSE) +
  scale_fill_manual(values = c("1" = "#A9D6E5"), na.value = NA) +  # Continent en gris, NA en blanc
  new_scale_fill() +
  geom_point(data = df_10m_clean, aes(x = lon, y = lat, color = light_20m), size = 2) +
  scale_color_viridis_c(limits = c(0, 0.1), oob = scales::squish, na.value = NA) +
  geom_point(aes(x = DDU_loc$lon, y = DDU_loc$lat), color = "red", shape = 4, size = 3, stroke = 1) +  # DDU
  annotate("text", x = DDU_loc$lon, y = DDU_loc$lat, label = "DDU", hjust = -0.2, vjust = -0.5, color = "red", size = 4) +  # DDU label
  xlab("Longitude") + ylab("Latitude") +
  theme_minimal() +
  coord_sf(xlim = c(139, 140.5),ylim = c(-66.8, -66.4))

```

<h4>Computing solar angle & classifying in Day / Night / Twilight by atitude bins</h4>
```{r SolarAngle}

# Calcul angle solaire
solar_angle <- sunAngle(dateTime, lon, lat)$altitude
solar_threshold <- -12

# Trancher la latitude tous les 0.05°
lat_bin <- floor(lat * 20) / 20

df_solar <- data.frame(
  date = dateTime,
  lat = lat,
  lon = lon,
  lat_bin = lat_bin,
  solar_angle = solar_angle,
  light = light_surf_avg,
  lat_bin_label = factor(paste0("[", lat_bin, "°, ", lat_bin + 0.05, "°["))
)

# Ajouter la période jour/crépuscule/nuit
df_solar <- df_solar %>%
  mutate(
    period = case_when(
      solar_angle >= 0 ~ "Day",
      solar_angle < 0 & solar_angle > solar_threshold ~ "Twilight",
      solar_angle <= solar_threshold ~ "Night"
    ))

# Angle solaire par tranche de latitude
p1 <- ggplot(df_solar, aes(x = date, y = solar_angle, color = period)) +
  geom_point() +
  facet_wrap(~lat_bin_label, scales = "fixed") +
  coord_cartesian(ylim = c(-40, 40)) +
  labs(
    title = "Solar Angle by Latitude Bin",
    x = "Date", y = "Solar Angle (°)"
  ) +
  scale_color_manual(values = c("Day" = "red", "Twilight" = "green", "Night" = "navy")) +
  theme_minimal()

# Lumière par tranche de latitude
p2 <- ggplot(df_solar, aes(x = date, y = light, color = period)) +
  geom_point() +
  facet_wrap(~lat_bin_label, scales = "free_y") +
  coord_cartesian(ylim = c(-5, 15)) +
  labs(
    title = paste0("Light (", SURFACE_NOT_NAN[1], " to ", SURFACE_NOT_NAN[7], "m deep) by Latitude Bin"),
    x = "Date", y = "ln(µmol/m²/sec)"
  ) +
  scale_color_manual(values = c("Day" = "red", "Twilight" = "green", "Night" = "navy")) +
  theme_minimal()

print(p1)
print(p2)

```

<h4>Plotting solar angle and light values at 4-10m deep</h4>
```{r LightThroughTime}

nPeriod = df_solar %>%  count(period)

cat("Number of Day values :", nPeriod$n[1], "\n")
cat("Number of Night values :", nPeriod$n[2], "\n")
cat("Number of Twilight values :", nPeriod$n[3], "\n")

ggplot(df_solar, aes(x = date, y = solar_angle, color = period)) +
  geom_point() +
  labs(title = "Solar Angle",
       x = "Date", y = "Solar Angle (°)") +
  scale_color_manual(values = c("Day" = "red", "Twilight" = "green", "Night" = "navy")) +
  theme_minimal()

vline_dates <- as.POSIXct(c("2019-02-20 01:12:00", "2019-03-21 00:46:00", "2019-04-19 07:30:00"))

ggplot(df_solar, aes(x = date, y = light, color = period)) +
  geom_point() +
  # geom_vline(xintercept = vline_dates, linetype = "dashed", color = "black") +
  # annotate("text", x = vline_dates[1], y = max(df_solar$light, na.rm = TRUE), label = "20/02\n🌙", angle = 90, vjust = -0.5, size = 3) +
  # annotate("text", x = vline_dates[2], y = max(df_solar$light, na.rm = TRUE), label = "21/03\n🌙", angle = 90, vjust = -0.5, size = 3) +
  # annotate("text", x = vline_dates[3], y = max(df_solar$light, na.rm = TRUE), label = "19/04\n🌙", angle = 90, vjust = -0.5, size = 3) +
  labs(
    title = paste0("Light (", SURFACE_NOT_NAN[1], " to ", SURFACE_NOT_NAN[7], "m deep)"),
    x = "Date", y = "ln(µmol/m²/sec)"
  ) +
  scale_color_manual(values = c("Day" = "red", "Twilight" = "green", "Night" = "navy")) +
  theme_minimal()
```

<h4>Geographical Classification of Profiles Relative to the DDU Station</h4>
```{r Light_profile_NSEW}
df_solar$lon <- lon
df_solar$lat <- lat

lon_center <-  DDU_loc$lon 
lat_center <-  DDU_loc$lat

df_solar <- df_solar %>%
  mutate(zone = case_when(
    lon <= lon_center & lat >= lat_center ~ "Nord-Ouest",
    lon > lon_center & lat >= lat_center ~ "Nord-Est",
    lon <= lon_center & lat < lat_center ~ "Sud-Ouest",
    lon > lon_center & lat < lat_center ~ "Sud-Est"
  ))

ggplot() +
  # bathy et continent
  geom_spatraster_contour(data = fine_topo_spat) +  # lignes de contour
  geom_spatraster_contour_text(data = fine_topo_spat) +  # texte des courbes
  geom_spatraster(data = continent_factor, show.legend = FALSE) +  # couche continent
  scale_fill_manual(values = c("1" = "slategray"), na.value = NA) +  # couleur du continent
  new_scale_fill() +
  geom_spatraster(data = glacier_factor,alpha = 0.9, show.legend = FALSE) +
  scale_fill_manual(values = c("1" = "#A9D6E5"), na.value = NA) +  # glacier
  # Données solaires
  geom_point(data = df_solar, aes(x = lon, y = lat, color = zone, shape = period), size = 3, alpha = 0.8) +
  # DDU
  geom_point(aes(x = DDU_loc$lon, y = DDU_loc$lat), color = "red", shape = 4, size = 3, stroke = 1) +
  annotate("text", x = DDU_loc$lon, y = DDU_loc$lat, label = "DDU", hjust = -0.2, vjust = -0.5, color = "red", size = 4) +

  labs(title = "Zones (couleur) et période jour/nuit/crépuscule (forme)", x = "Longitude", y = "Latitude") +
  scale_color_manual(values = c("Nord-Ouest" = "darkorange", "Nord-Est" = "purple", "Sud-Ouest" = "blue", "Sud-Est" = "darkgreen")) +
  scale_shape_manual(values = c("Day" = 16, "Twilight" = 17, "Night" = 15)) +
  theme_minimal() +
  coord_sf(xlim = c(139, 140.5),ylim = c(-66.8, -66.4))
```

<h4>Classification of Profiles Based on Their Distance from the DDU Station</h4>
```{r LightProfile_distance_DDU}
library(geosphere)

df_solar <- df_solar %>%
  mutate(distance_to_center_km = distHaversine(cbind(lon, lat), c(lon_center, lat_center)) / 1000)  # en km

df_solar <- df_solar %>%
  mutate(distance_group = case_when(
    distance_to_center_km < 10 ~ "Centre",
    distance_to_center_km < 25 ~ "Périphérie proche",
    TRUE ~ "Périphérie lointaine"
  ))

ggplot() +
  # bathy et continent
  geom_spatraster_contour(data = fine_topo_spat) +
  geom_spatraster_contour_text(data = fine_topo_spat) +
  geom_spatraster(data = continent_factor, show.legend = FALSE) +
  scale_fill_manual(values = c("1" = "slategray"), na.value = NA) +
  new_scale_fill() +
  geom_spatraster(data = glacier_factor,alpha = 0.9, show.legend = FALSE) +  # glacier
  scale_fill_manual(values = c("1" = "#A9D6E5"), na.value = NA) +
  # Groupes de distance
  geom_point(data = df_solar, aes(x = lon, y = lat, color = distance_group, shape = period), size = 2, alpha = 0.8) +
  # DDU
  geom_point(aes(x = DDU_loc$lon, y = DDU_loc$lat), color = "red", shape = 4, size = 3, stroke = 1) +
  annotate("text", x = DDU_loc$lon, y = DDU_loc$lat, label = "DDU", hjust = -0.2, vjust = -0.5, color = "red", size = 4) +

  labs(title = "", x = "Longitude", y = "Latitude") + scale_color_manual(values = c("Centre" = "firebrick", "Périphérie proche" = "goldenrod", "Périphérie lointaine" = "steelblue")) +
  scale_shape_manual(values = c("Day" = 16, "Twilight" = 17, "Night" = 15)) +
  theme_minimal() +
  coord_sf(xlim = c(139, 140.5),ylim = c(-66.8, -66.4))
```

<h4> Extracting suspicious night profiles</h4>
```{r extract Suspicious solar profiles}
# CODE POUR LA PLUS FORTE VALEUR DANS LA NUIT
# pas de points categorisés comme nuit pour cet individu.
# max_night_light <- df_solar %>%
#   filter(period == "Night") %>%
#   summarise(max_light = max(light, na.rm = TRUE)) %>%
#   pull(max_light)
# 
# night_rows <- which(df_solar$period == "Night")
# local_max_index <- which.max(df_solar$light[night_rows])
# global_max_index <- night_rows[local_max_index]

global_max_index <- 10


plot(light[,global_max_index], pres_vec,
     type = "l",
     xlim = range(light[,global_max_index], na.rm = TRUE),
     ylim = rev(c(0, 50)),
     xlab = "Light ln(µmol/m²/s)",
     ylab = "Depth (m)",
     main = paste0("profil index: ", global_max_index, " (", format(dateTime[global_max_index], "%Y-%m-%d"), ")"))
grid()

```

<h2>RECAP TABLE</h2>
<h4>Extracting tab and gathering data from Previous codes</h4>
```{r final_SIC_fastIce_tab}

dt_ddu <- format(profiles_per_month, tz = "Australia/Hobart", usetz = TRUE)

points_sf <- st_as_sf(profiles_per_month, coords = c("lon", "lat"), crs = 4326) # Transforme en objets spatiaux
points_vect <- terra::vect(points_sf)

glacier_mask <- glacier
glacier_mask[glacier_mask == 4] <- 1

amsr_rast <- rast(amsr_raster)  # conversion RasterLayer -> SpatRaster

extract_sic <- terra::extract(amsr_rast, points_vect, ID = FALSE)
extract_mean   <- terra::extract(mean_spat, points_vect, ID = FALSE)
extract_median <- terra::extract(median_spat, points_vect, ID = FALSE)
extract_sd     <- terra::extract(sd_spat, points_vect, ID = FALSE)
extract_glac   <- terra::extract(glacier_mask, points_vect, ID = FALSE)

profiles_per_month$SICm        <- extract_sic[,1]
profiles_per_month$SICm_mean   <- extract_mean[,1]
profiles_per_month$SICm_median <- extract_median[,1]
profiles_per_month$SICm_sd     <- extract_sd[,1]
profiles_per_month$glacier_presence <- extract_glac[,1]


profiles_per_month
```

<h2>Other</h2>
<h4>Cemetery : Unused code, but may be useful one day.</h4>
```{r Cemetery}
# COUCHE EUPHO
# pas utile pour le moment (attention code peut etre faux)

# # Calculer Zeu : première profondeur où la lumière < 1% (log-scale)
# zeuDep_temp <- apply(light, 2, function(col) {
#   idx <- which(col < -log(100))[1]  # première position vraie
#   if (is.na(idx)) NA else idx       # si rien trouvé, mettre NA
# })
# 
# # Remplacer les 0 (si jamais) par NA (équivalent MATLAB)
# zeuDep_temp[zeuDep_temp == 0] <- NA
# 
# # Stocker dans la structure lightData
# Zeu <- zeuDep_temp


# PAR 15
# light_non_log <- exp(light_corrected)
# par15_depths <- rep(NA, ncol(light_non_log))  # vecteur pour stocker les profondeurs
# 
# for (i in 1:ncol(light_non_log)) {
#   below15 <- which(light_non_log[, i] < 15)
#   if (length(below15) > 0) {
#     # On prend la première profondeur où light < 15
#     par15_depths[i] <- pres[min(below15), i]
#   }
# }

# RSD
# # Calcul écart type et moyenne)
#   stdChla <- sd(chla_profile, na.rm = TRUE)
#   meanChla <- mean(chla_profile, na.rm = TRUE)
# 
#   # Calcul du RSD si la moyenne est non nulle
#   if (!is.na(meanChla) && meanChla != 0) {
#     rsd[ii_temp] <- stdChla / meanChla
#   }

```
